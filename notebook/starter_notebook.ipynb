{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Intro to Machine Learning with TritonHacks 2024!\n",
    "\n",
    "### Importing Packages\n",
    "Import statements allow us to use library and built-in functions within the library so we can utilize them!\\\n",
    "First, we are going to import packages that are going to be very basic for the Data Science Project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the data. For now, let's use the given structure and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Google Colab to edit this Notebook, please uncomment the code block below so that you can upload `train_extended.csv` to the workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `read_csv()` function will allow us to load the dataset in `.csv` format into the format called DataFrame, which is a super common form of data in Data Science.\\\n",
    "You do not have to follow our structure entirely and may replace the string in `read_csv()` as necessary with the anticipated path.\\\n",
    "The `head()` function is a built in function within the Dataframe which only lets you see the top portion of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Population</th>\n",
       "      <th>Precipitation (in.)</th>\n",
       "      <th>Temperature (°F)</th>\n",
       "      <th>Air Quality (AQI)</th>\n",
       "      <th>Population Density (per sq. mi)</th>\n",
       "      <th>Land Type</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Has Park</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carlsbad</td>\n",
       "      <td>92009</td>\n",
       "      <td>114160</td>\n",
       "      <td>12.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1236.1</td>\n",
       "      <td>3</td>\n",
       "      <td>33.1581</td>\n",
       "      <td>117.3506</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>San Diego</td>\n",
       "      <td>92154</td>\n",
       "      <td>1381182</td>\n",
       "      <td>10.3</td>\n",
       "      <td>64.7</td>\n",
       "      <td>2</td>\n",
       "      <td>829.6</td>\n",
       "      <td>3</td>\n",
       "      <td>32.7157</td>\n",
       "      <td>117.1611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>National City</td>\n",
       "      <td>91950</td>\n",
       "      <td>61394</td>\n",
       "      <td>11.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3059.5</td>\n",
       "      <td>3</td>\n",
       "      <td>32.6781</td>\n",
       "      <td>117.0992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ocotillo Wells</td>\n",
       "      <td>92004</td>\n",
       "      <td>3200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "      <td>33.1495</td>\n",
       "      <td>116.1531</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Julian</td>\n",
       "      <td>92036</td>\n",
       "      <td>3075</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0781</td>\n",
       "      <td>116.6006</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             City  Zip Code  Population  Precipitation (in.)  \\\n",
       "0        Carlsbad     92009      114160                 12.0   \n",
       "1       San Diego     92154     1381182                 10.3   \n",
       "2   National City     91950       61394                 11.0   \n",
       "3  Ocotillo Wells     92004        3200                  3.0   \n",
       "4          Julian     92036        3075                 30.0   \n",
       "\n",
       "   Temperature (°F)  Air Quality (AQI)  Population Density (per sq. mi)  \\\n",
       "0              63.0                  2                           1236.1   \n",
       "1              64.7                  2                            829.6   \n",
       "2              64.0                  2                           3059.5   \n",
       "3              74.0                  1                              2.2   \n",
       "4              54.0                  1                              7.4   \n",
       "\n",
       "   Land Type  Latitude  Longitude  Has Park  \n",
       "0          3   33.1581   117.3506         1  \n",
       "1          3   32.7157   117.1611         1  \n",
       "2          3   32.6781   117.0992         1  \n",
       "3          1   33.1495   116.1531         1  \n",
       "4          1   33.0781   116.6006         1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../sandiego.csv') # Modify the file path inside the .read_csv() function as necessary. \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look of the columns we have right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['City', 'Zip Code', 'Population', 'Precipitation (in.)',\n",
       "       'Temperature (°F)', 'Air Quality (AQI)',\n",
       "       'Population Density (per sq. mi)', 'Land Type', 'Latitude', 'Longitude',\n",
       "       'Has Park'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the column `id` is just an index, which won't be helpful. Therefore, we will drop that column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zip Code</th>\n",
       "      <th>Population</th>\n",
       "      <th>Precipitation (in.)</th>\n",
       "      <th>Temperature (°F)</th>\n",
       "      <th>Air Quality (AQI)</th>\n",
       "      <th>Population Density (per sq. mi)</th>\n",
       "      <th>Land Type</th>\n",
       "      <th>Has Park</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92009</td>\n",
       "      <td>114160</td>\n",
       "      <td>12.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1236.1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>92154</td>\n",
       "      <td>1381182</td>\n",
       "      <td>10.3</td>\n",
       "      <td>64.7</td>\n",
       "      <td>2</td>\n",
       "      <td>829.6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91950</td>\n",
       "      <td>61394</td>\n",
       "      <td>11.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3059.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92004</td>\n",
       "      <td>3200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92036</td>\n",
       "      <td>3075</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Zip Code  Population  Precipitation (in.)  Temperature (°F)  \\\n",
       "0     92009      114160                 12.0              63.0   \n",
       "1     92154     1381182                 10.3              64.7   \n",
       "2     91950       61394                 11.0              64.0   \n",
       "3     92004        3200                  3.0              74.0   \n",
       "4     92036        3075                 30.0              54.0   \n",
       "\n",
       "   Air Quality (AQI)  Population Density (per sq. mi)  Land Type  Has Park  \n",
       "0                  2                           1236.1          3         1  \n",
       "1                  2                            829.6          3         1  \n",
       "2                  2                           3059.5          3         1  \n",
       "3                  1                              2.2          1         1  \n",
       "4                  1                              7.4          1         1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['City', 'Latitude', 'Longitude'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "Let's start with the most basic model! We are going to be use the linear regression model from `scikit-learn`.\\\n",
    "This will be our very base model and your goal is to build a model that performs better!\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "Linear Regression is a very fundamental algorithm in statistics. The simplest way to explain it is to find the \"line of best fit\".\\\n",
    "There will be various factors (which we call \"variables\") such as length, diameter, etc., and the model will determine the age based on those factors.\\\n",
    "As a data scientist, one of our main jobs is to find the relevant variables based on given information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first, we will start by importing the pre-built model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Linear Regression is technically a mathematical equation (or prompt), we will drop the column `sex` because it is a string value and not a number.\\\n",
    "(However, sex might be relavant information that we want to utilize. We will teach you how to utilize them later on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save data without the `sex` column in a seperate DataFrame called `data_no_sex`, since we might still want to have access to the full data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model\n",
    "\n",
    "What we are going to do now is split our dataset into 2 different sets: a training set and a test set. What are these?\n",
    "\n",
    "A training set is used to literally \"train\" the model. The model will understand the pattern between given variables and determine the value.\\\n",
    "A test set is used to \"test\" how well our model perform. Using the given function of `scikit-learn`, we will split them into a training set and a test set.\n",
    "\n",
    "The `test_size` parameter refers the the proportion (size) of the test dataset out of the whole dataset.\\\n",
    "`random_state` is setting a seed. Though `train_test_split` is supposed to be a random function, it actually sees if you are making progress,\\\n",
    "You might want to compare outputs under the same conditions. To do so, we are setting a \"seed\" (like Minecraft!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent and Dependent Variables\n",
    "\n",
    "Independent variables are what we expect will influence dependent variables. A dependent variable is what happens as a result of the independent variable.\\\n",
    "In a machine learning project, we want to 'predict' the dependent variable (or so called 'Y' of the function) using independent variables (or so called 'X').\\\n",
    "In the crab age dataset, we want to predict the age, so age will be the dependent variable (or y) and the rest will be independent variables (or X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Has Park']\n",
    "X = data.drop(['Has Park'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n",
      "64\n",
      "42\n",
      "22\n",
      "42\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `fit()` function generates the trained model with the given dataset.\n",
    "\n",
    "The `score()` function shows how \"well\" the model fits the data.\n",
    "\n",
    "A value of 0 shows that the model is completely random (i.e., it can't find any correlation between independent and dependent variables), while a value of 1 is saying the model found a perfect relationship between the independent and dependent variables.\n",
    "We will measure how well the model performs on both the training set and the testing set. \n",
    "\n",
    "Note: It is important to realize that the `score()` function is NOT a reliable indicator of the improvement of the model, since it only measures how well the model fits the data. Other error functions, such as RMSE, MSE, and MAE are better indicators of how well a model is predicting, since we want it to be as accurate as possible (more discussion about these error functions can be found later in the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5821425014064188"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression().fit(X_train, y_train)\n",
    "model.score(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13315857513410723"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has a slightly lower score on the test set compared to the score on the training set. Why?\\\n",
    "A: It is because the model is trained based on the training set which means the model is \"optimized\" for the training set and the optimized model is \"guessing\" based on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to view coefficients (which shows that \"relationship\", or how much each independent variable affects the dependent variable)\\\n",
    "and the intercept (in this case, the y-intercept because we are trying to find the \"line of best fit\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.10299906e-04  3.86968361e-08  4.27733217e-02  5.88541247e-02\n",
      "  1.04331769e-01  1.72708845e-05  3.72398918e-01]\n",
      "33.15901348170644\n"
     ]
    }
   ],
   "source": [
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this is the time to see the actual prediction values that our model is generating for both training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.37843079,  1.12250266,  0.13676957,  0.2998153 ,  0.27504601,\n",
       "        0.69909063,  1.14175213,  0.20597636,  1.00683524,  0.99029862,\n",
       "        1.12255094,  0.55819827,  0.2161476 ,  1.00951509,  0.5145994 ,\n",
       "        0.94910073,  1.15329865,  0.29987283,  1.16743821,  0.69439189,\n",
       "        1.17252287, -0.00174632,  0.91261538,  1.11906849,  0.95692148,\n",
       "        0.65574398,  0.88518279,  1.01736229,  0.38406836,  0.23340636,\n",
       "        0.94207787,  0.3837398 ,  0.7882116 ,  0.26619622,  0.39626816,\n",
       "        0.26289311,  0.82105847,  0.78566196,  0.3349512 ,  0.34884411,\n",
       "        0.28444361,  1.10887728])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prediction = model.predict(X_train)\n",
    "train_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84006944, 1.0900952 , 0.12688864, 1.01533072, 1.05018396,\n",
       "       0.18562649, 0.27706108, 0.81705502, 0.9230729 , 0.93918554,\n",
       "       0.9074415 , 0.8817956 , 0.21296306, 1.05673582, 1.02925206,\n",
       "       1.1083979 , 0.22420346, 0.37019864, 1.09675366, 0.25148389,\n",
       "       0.87639764, 0.98044561])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction = model.predict(X_test)\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common ways to quantify the error of the model is using the Root Mean Squared Error (RMSE).\\\n",
    "Under given predictions of the model, we take each actual (observed) value and calculate the difference between it and the prediction value, square the differences, calculate the mean of the squared differences, and take the square root of it.\n",
    "\n",
    "This function will allow us to calculate RMSE (the cool thing about the `NumPy` array is that it works as a \"vector\" or \"matrix\". We do not need to use any iteration and can just treat it as a number).\n",
    "\n",
    "We recommend you to use this function to measure the performance of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y,pred):\n",
    "    return np.sqrt(np.mean((y-pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30472482982140575"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_RMSE = RMSE(y_train, train_prediction)\n",
    "train_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41465104513264955"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_RMSE = RMSE(y_test, test_prediction)\n",
    "test_RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait...\n",
    "If you look back, the predictions are decimal numbers...\n",
    "but is it possible to have decimal number as your age? (unless you want to be specific and sound like a weirdo).\n",
    "Therefore, we will use `round()` to round the number to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## [ ... for i in ...] is called list comprehension syntax in Python.\n",
    "## https://www.w3schools.com/python/python_lists_comprehension.asp refer to this link if you want to know futher about it\n",
    "prediction = np.array([round(i) for i in model.predict(X_train)])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is giving us integer values. Let's see how off we are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3450327796711771"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RMSE(y_train,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the accuracy on our test set? (This is what we ACTUALLY want to know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4767312946227962"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.array([round(i) for i in model.predict(X_test)])\n",
    "RMSE(y_test,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Data\n",
    "\n",
    "We can first visualize our data by creating scatterplots between independent variables and dependent variable(crab age). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the scatterplot\n",
    "\n",
    "# Colors of plots: maroon = male, peru = female, wheat = indeterminate\n",
    "colors = [\"maroon\", \"peru\", \"wheat\"]\n",
    "\n",
    "for var in categories[:-1]:\n",
    "\n",
    "    # Title/axis of plot\n",
    "    plt.title(\"{} to Age\".format(var))\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel(\"Age\")\n",
    "\n",
    "    for i, gender in enumerate(all_data):\n",
    "        # Get age data, (dependent variable)\n",
    "        y = all_data[gender]['Age']\n",
    "\n",
    "        # Get data without sex, and age\n",
    "        X = all_data[gender].drop(['Sex','Age'],axis=1)\n",
    "\n",
    "        # Plot data for specific gender\n",
    "        plt.scatter(X[var], y, c = colors[i])\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you seeing any strong correlation?(to determine those, we want to see if the categorical variable affects the distribution of the value(how scatter or each dot is distributed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Mean Value of Variables\n",
    "\n",
    "We can start exploring the data by first looking at each variable based on categorical values. In this case, the categorical value is the crab's sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avgs = {cat : [] for cat in categories}\n",
    "all_errors = {cat : [] for cat in categories}\n",
    "\n",
    "for var in categories:\n",
    "\n",
    "    # Get data specific to variable being considered\n",
    "    male_data_var = np.array(data_male_only[var])\n",
    "    female_data_var = np.array(data_female_only[var])\n",
    "    indeterminate_data_var = np.array(data_indeterminate_only[var])\n",
    "\n",
    "    # Calculate average using NumPy's in-built functions\n",
    "    avg_male = np.mean(male_data_var)\n",
    "    error_male = np.std(male_data_var)\n",
    "\n",
    "    avg_female = np.mean(female_data_var)\n",
    "    error_female = np.std(female_data_var)\n",
    "\n",
    "    avg_indeterminate = np.mean(indeterminate_data_var)\n",
    "    error_indeterminate = np.std(indeterminate_data_var)\n",
    "\n",
    "    # Store mean/error in list\n",
    "    all_avgs[var] = (avg_male, avg_female, avg_indeterminate)\n",
    "    all_errors[var] = (error_male, error_female, error_indeterminate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gender in enumerate(all_data):\n",
    "    print(\"{} Only Data\".format(gender))\n",
    "\n",
    "    for var in categories:\n",
    "        print(\"Variable {} Average: {:.3f}\".format(var, all_avgs[var][i]))\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there are some differences in the mean values of each category so it seems like this category has some relationship to the crab age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Mean Value of Variables\n",
    "\n",
    "We can use `Matplotlib` to plot the mean value of each variable based on the crab's sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart visualization\n",
    "\n",
    "for var in categories:\n",
    "    fig = plt.figure(figsize = (10, 5))\n",
    "    \n",
    "    # creating the bar plot\n",
    "    bar_plt = plt.bar(all_data.keys(), list(all_avgs[var]), width = 0.4)\n",
    "    plt.errorbar(all_data.keys(), list(all_avgs[var]), all_errors[var], linestyle='None', marker='o')\n",
    "    \n",
    "    bar_plt[0].set_color(\"maroon\")\n",
    "    bar_plt[1].set_color(\"peru\")\n",
    "    bar_plt[2].set_color(\"wheat\")\n",
    "\n",
    "    plt.xlabel(\"Gender\")\n",
    "    plt.ylabel(\"Mean Value of {}\".format(var))\n",
    "    plt.title(\"{} per Gender\".format(var))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Correlation Between Variables\n",
    "Now that we have a basic understanding of the data, let's look for correlations between different variables. Since we want to predict the average age, we'll experiment with data from columns 3 to 9 ('length' through 'shell weight'). This will be the X variable and we can set age as the y variable. We use Pearson R to find the correlation coefficient. R values closer to 1 or -1 have a strong correlation, while an R value of 0 has no correlation.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gender in all_data:\n",
    "    print(\"Correlation Coefficients {}\".format(gender))\n",
    "    for var in categories[:-1]:\n",
    "        X = list(all_data[gender][var])\n",
    "        y = list(all_data[gender]['Age'])\n",
    "\n",
    "        corr, _ = pearsonr(X,y)\n",
    "\n",
    "        print('{}: {:.3f}'.format(var, corr))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Correlation\n",
    "\n",
    "We can use the scatterplot from `Matplotlib` to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for var in categories[:-1]:\n",
    "\n",
    "    # Title and labels of plot\n",
    "    plt.title(\"{} to Age\".format(var))\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel(\"Age\")\n",
    "\n",
    "    # Limits of axis (used to scale plots)\n",
    "    X_max = 0\n",
    "    y_max = 0\n",
    "    \n",
    "    for i, gender in enumerate(all_data):\n",
    "\n",
    "        y = all_data[gender]['Age'] # Get age data, (dependent variable)\n",
    "\n",
    "        X = all_data[gender].drop(['Sex','Age'],axis=1) # Get data without  sex, and age\n",
    "\n",
    "        # Update max X/y of plot\n",
    "        X_max = max(np.max(X[var]), X_max)\n",
    "        y_max = max(np.max(y), y_max)\n",
    "        \n",
    "        lsq = np.polyfit(X[var], y, 1) # Calculate line of best fit\n",
    "\n",
    "        plt.scatter(X[var], y, c = colors[i], alpha = 0.5) # Plot points on scatterplot\n",
    "\n",
    "        x_extend = np.linspace(0, 100, 100) # Extend line of best fit to edge of graph\n",
    "\n",
    "        plt.plot(x_extend, np.polyval(lsq, x_extend), color = colors[i], linestyle = '-', linewidth = 2) # Draw line of best fit\n",
    "\n",
    "    # Set limit of plot axis and show plot\n",
    "    plt.xlim(0, X_max + 0.1 * X_max)\n",
    "    plt.ylim(0, y_max + 0.1 * y_max)\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers\n",
    "\n",
    "In the above example, there are clearly data points that lie far outside the range of the majority. To fix this, we can remove any data that is outside 1.5 * IQR (interquartile range) above Q3 or below Q1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in categories[:-1]:\n",
    "    for gender in all_data:\n",
    "        X = all_data[gender].drop(['Sex','Age'],axis=1) # Get data without sex, and age\n",
    "\n",
    "        q3, q1 = np.percentile(X[var], [75,25]) # Calculate quartiles \n",
    "        iqr = q3 - q1; # Find IQR\n",
    "\n",
    "        # Remove values that lie 1.5 * IQR beyond Q1 and Q3\n",
    "        all_data[gender] = all_data[gender][(X[var] >= q1 - 1.5 * iqr) & (X[var] <= q3 + 1.5 * iqr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recomputing Correlation\n",
    "We can recompute the correlation with the outliers removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gender in all_data:\n",
    "    print(\"Correlation Coefficients {}\".format(gender))\n",
    "    for var in categories[:-1]:\n",
    "        X = all_data[gender][var]\n",
    "        y = all_data[gender]['Age']\n",
    "        corr, _ = pearsonr(X,y)\n",
    "\n",
    "        print('{}: {:.3f}'.format(var, corr))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Correlation (without outliers)\n",
    "\n",
    "And now, we redo our plot above with the outliers removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with outliers removed\n",
    "\n",
    "for var in categories[:-1]:\n",
    "\n",
    "    # Title and labels of plot\n",
    "    plt.title(\"{} to Age\".format(var))\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel(\"Age\")\n",
    "\n",
    "    # Limits of axis (used to scale plots)\n",
    "    X_max = 0\n",
    "    y_max = 0\n",
    "    \n",
    "    for i, gender in enumerate(all_data):\n",
    "\n",
    "        y = all_data[gender]['Age'] # Get age data, (dependent variable)\n",
    "\n",
    "        X = all_data[gender].drop(['Sex','Age'],axis=1) # Get data without sex, and age\n",
    "\n",
    "        # Update max X/y of plot\n",
    "        X_max = max(np.max(X[var]), X_max)\n",
    "        y_max = max(np.max(y), y_max)\n",
    "        \n",
    "        lsq = np.polyfit(X[var], y, 1) # Calculate line of best fit\n",
    "\n",
    "        plt.scatter(X[var], y, c = colors[i], alpha = 0.5) # Plot points on scatterplot\n",
    "\n",
    "        x_extend = np.linspace(-100, 100, 100) # Extend line of best fit to edge of graph\n",
    "\n",
    "        plt.plot(x_extend, np.polyval(lsq, x_extend), color = colors[i], linestyle = '-', linewidth = 2) # Draw line of best fit\n",
    "\n",
    "    # Set limit of plot axis and show plot\n",
    "    plt.xlim(0, X_max + 0.1 * X_max)\n",
    "    plt.ylim(0, y_max + 0.1 * y_max)\n",
    "    plt.show()\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like there might be some variables that has some relationships with the dependent variable. Is there any other possible ways that is more obvious then just a visualization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANOVA\n",
    "\n",
    "Analysis of variance (ANOVA) is a statistical test used to evaluate the difference between the means of more than two groups. This statistical analysis tool separates the total variability within a data set into two components: random and systematic factors.\n",
    "For instance, does the sex has statisgically significant relationship with the other crab variables? Let's see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see if categorical variable (in this case, sex) has any significance to be added to the model's variable. In order to do so, seperate our data with each category. Then, run the test using f_oneway(this is the scipy's function that runs one way anova) on the variable that we are curious about to see the p-value. \\\n",
    "As p-value is lower, it means that the difference between each group is less likely to be caused by random factor(which means it is more likely to have some meanings or significance). \\\n",
    "In our case, let's see if sex has anything to do with the wieght of the crab.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "all_data = {\"Male\" : data_male_only, \"Female\" : data_female_only, \"Indeterminate\" : data_indeterminate_only}\n",
    "f_oneway(all_data['Male']['Weight'],all_data['Female']['Weight'], all_data['Indeterminate']['Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P-value in this case is 0(which is not actually 0 but implies that it is extremely low) so we know weight is statistically significant!\\\n",
    "You can use this to any variables and try yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "Earlier, we mentioned how nominal data cannot be accounted for numerically because it is by definition, non-numerical. In order to factor in nominal data from our linear regression model, we must encode these categories, like sex, into numbers. To do this, we can use the One Hot Encoder from `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output = False).set_output(transform='pandas')\n",
    "ohetransform = ohe.fit_transform(data[['Sex']])\n",
    "ohetransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's append these numerical values for sex into our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoded = pd.concat([data, ohetransform], axis = 1).drop(columns = ['Sex', 'Sex_0.025'])\n",
    "data_encoded.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our updated dataset, let's see if encoding sex improves the accuracy of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_encoded['Age']\n",
    "X = data_encoded.drop(['Age'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=21)\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "model_with_sex = model.score(X_train, y_train)\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to find the RMSE for the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction = model.predict(X_train)\n",
    "train_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model.predict(X_test)\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RMSE = RMSE(y_train, train_prediction)\n",
    "train_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RMSE = RMSE(y_test, test_prediction)\n",
    "test_RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This improved the model performance slightly and you might have already expected that if you performed EDA and different test to see if sex has any statistical significancy. If you are using different dataset, please always prove and check to see if your conclusion in EDA aligns with the model performance.\n",
    "\n",
    "Now, let's take a look at a another method to enhance model performance and improve the interpretability of the data: normalization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization Techniques \n",
    "\n",
    "Data normalization is a data preprocessing technique which rescales the numerical features of a dataset to a standard range. In other words, we want to \"level the playing field\" for all variables so that certain variables do not dominate others trivially.\n",
    "\n",
    "First, we will show linear normalization, otherwise known as \"min-max scaling\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min-Max Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "NormData = scaler.fit_transform(data_encoded)\n",
    "\n",
    "NormData = pd.DataFrame(NormData, columns = data_encoded.columns)\n",
    "NormData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear normalization transforms each data point to a value within the range of 0 to 1. This crucial preprocessing step ensures that the magnitudes of various data categories are uniformly scaled, preventing any potential misinterpretation of their relative importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-Score Normalization\n",
    "\n",
    "Often called standardization, the z-score method transforms a dataset to have a mean value of 0 and deviation of 1. This makes the data more digestible for certain ML algorithms, such as principal component analysis (PCA), and mitigates the effect that outliers have on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we will perfrom 2 differnt normalization methods without incoperating sex variable to see if normalization improves the model from the baseline result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_z_scaled = data_encoded.copy() \n",
    "\n",
    "## Standardization is applied only to continues numerical values, so here we drop binary encoding for sex\n",
    "df_z_scaled = df_z_scaled.drop(columns = ['Sex_F', 'Sex_I', 'Sex_M'])\n",
    "\n",
    "## We subtract the avg value of each feature from every data point, and then divide by the amount that it deviates from the average\n",
    "for column in df_z_scaled.columns: \n",
    "\tdf_z_scaled[column] = (df_z_scaled[column] - df_z_scaled[column].mean()) / df_z_scaled[column].std()\t \n",
    " \n",
    "df_z_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_z_scaled['Age'] # try min max scale to see which one improves your model better\n",
    "X = df_z_scaled.drop(['Age'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=21)\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now to find the RMSE for the training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prediction = model.predict(X_train)\n",
    "train_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = model.predict(X_test)\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RMSE = RMSE(y_train, train_prediction)\n",
    "train_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_RMSE = RMSE(y_test, test_prediction)\n",
    "test_RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the normalization improved your model? If it did,(or did not) try to combine one hot encoding on your categorical variable with normalization on numerical variables together to see if the model performs better(DO NOT normalize one hot encoded values, combine after other numerical variables are normalized)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the Model Overfitting?\n",
    "\n",
    "What other tests can we run on our model? One thing we can do is test for overfitting. This is when a model 'adjusts' to predict training data, but is unable to predict new data. To test if our model is overfitting, we can look at error values: if the training data has a low error rate but the test data has a high error rate, this indicates that the model is overfitting.\n",
    "\n",
    "## Mean Squared Error, Mean Absolute Error\n",
    "There are different ways to calculate error. Remember that we already calculated RMSE (root mean squared error). RMSE is just the square root of Mean Squared Error, which is the average of the squared differences between our predicted and actual values. There is also MAE, or Mean Absolute Error. MAE is essentially the same, except we take the absolute value of the differences without squaring. Here's a quick breakdown of their pros and cons:\n",
    "### MSE:\n",
    "It is harder to interpret, since it is in units that are the square of the data. Like RMSE, it penalizes large errors more, making it more sensitive to outliers.\n",
    "### RMSE:\n",
    "It is easy to interpret since it shares the same units as the data, so it is more widely used. Like MSE, it penalizes large errors more, making it more sensitive to outliers.\n",
    "### MAE:\n",
    "It is easy to understand and interpret!\\\n",
    "Use MAE if you want to consider outliers less- It is not as sensitive to outliers, since it considers all errors with equal weight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's go back to our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_sex =  data.drop(['Sex'],axis=1)\n",
    "y = data_no_sex['Age']\n",
    "X = data_no_sex.drop(['Age'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=21)\n",
    "\n",
    "model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "prediction = np.array([round(i) for i in model.predict(X_train)])\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap, here is the RMSE function we wrote earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(y,pred):\n",
    "    return np.sqrt(np.mean((y-pred)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create functions for MSE and MAE ourselves! MSE will be like the RMSE function we wrote, except without the square root. And for our MAE function, we need to replace the square with an absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y,pred):\n",
    "    return np.mean((y-pred)**2)\n",
    "    \n",
    "def MAE(y,pred):\n",
    "    return np.mean(abs(y-pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's calculate the MSE and MAE for our model on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array([round(i) for i in model.predict(X_train)])\n",
    "MSE(y_train,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE(y_train,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's calculate the MSE and MAE for our model on testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array([round(i) for i in model.predict(X_test)])\n",
    "MSE(y_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE(y_test,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found that the error slightly increased on testing data. This is fairly normal, but could indicate possible overfitting. To do further testing, we can try something called K-fold cross validation.\n",
    "## Cross-Validation\n",
    "Before we created our model, we had to divide our data into a training set and a testing set. If we want to test our model more, we can use cross-validation.\n",
    "Cross-validation shuffles the data and splits the data into \"k\" groups, or folds. For example, if k = 10, it is called 10-Fold Cross Validation. First, a single fold is chosen to be the testing group: the model is trained on the other folds, then tested on our group. This is repeated \"k\" times for all the folds!\n",
    "\n",
    "![K-fold_image](../images/K-Fold_Cross-Validation.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 10)\n",
    "score_ten = cross_val_score(model,X,y, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"10-Fold Cross Validation Scores are: {}\".format(score_ten))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully got the scores for all 10 times it ran! Let's look at the average score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average 10-Fold Cross Validation score: {}\".format(score_ten.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to the score we calculated in the Introduction, and so we know our evaluation of our model performance is fairly accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation and RMSE\n",
    "We also want to perform k-fold cross validation using RMSE too, so we know what our average error is. Let's do this by changing the `scoring` parameter in `cross_val_score()`, and setting it equal to `neg_root_mean_squared_error`. Let's also make sure to take the absolute value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_score = abs(cross_val_score(model,X,y, scoring = 'neg_root_mean_squared_error', cv=kfold))\n",
    "print(\"RMSE Average 10-Fold Cross Validation Error: {}\".format(rmse_score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, we can now assess our model with both cross validation and RMSE!\n",
    "### Why is it important to test with both?\n",
    "Well, cross validation ensures that we are testing the model on new data each time--we switch up what the \"testing\" and \"training data\" are so that we can test our model for overfitting. Also, using an error measure like RMSE evaluates how big our errors are compared to the average. The RMSE isn't our only error measure though; we can use the MAE if we want to consider the outliers less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always try to utilize cross validation or k-fold method to prevent your model to overfit no matter what kinds of dataset you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've been using our linear regression model. Let's try using some other machine learning algorithms and running cross validation on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors Regression\n",
    "K-nearest neighbors is another common algorithm we can try. If you imagine all the data points on a graph, it classifies a new point by looking at the \"k\" number of nearest points. Basically, it groups together nearby points. \n",
    "\n",
    "![K-nearest_Neighbor](../images/K-nearest_Neighbors.png)\n",
    "\n",
    "### Features/Pros:\n",
    "-It can be used for classification or regression- in this case, we are doing regression! (Our model is looking for numerical ages, whereas classification would be grouping into separate categories.)\\\n",
    "-It is very sensitive to outliers.\\\n",
    "-You don't have to \"train\" the model, so it is flexible to data changes.\\\n",
    "-It is simple to implement and understand!\n",
    "\n",
    "### Cons:\n",
    "-It can be a slow and inefficient if there is a lot of data.\\\n",
    "-It can be hard to find the optimal number of neighbors.\\\n",
    "-Data needs to be balanced over the testing variable for the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the pre-built model and test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsRegressor()\n",
    "rmse_score = abs(cross_val_score(knn_model,X,y, scoring = 'neg_root_mean_squared_error', cv=kfold))\n",
    "print(\"RMSE Average 10-Fold Cross Validation Error: {}\".format(rmse_score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default KNN regressor from SKlearn uses k=5 as a default. Try to change the values of K to see which value works the best in your case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsRegressor(n_neighbors = 6) # you can change n_neighbors to whatever number that you want to see which one works better\n",
    "rmse_score = abs(cross_val_score(knn_model,X,y, scoring = 'neg_root_mean_squared_error', cv=kfold))\n",
    "print(\"RMSE Average 10-Fold Cross Validation Error: {}\".format(rmse_score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform KNN model training on preprocessed data(normalization, one hot encoding, etc.). If you are also curious different types of hyperparameter other than n_neighbors, please look into this [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "Decision tree is another popular algorithm we can test. It creates a tree map that splits into more and more nodes as it takes into account the different variables. Here is a a tree visualization, with each decision splitting each \"branch\" into two nodes:\n",
    "\n",
    "![decisiontree1](../images/DecisionTree1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, each \"decision\" that splits a node is like drawing a line segment dividing the data into two groups. In the graph below, each line segment corresponds to a split. \n",
    "\n",
    "![decisiontree1.png](../images/DecisionTree2.png)\n",
    "After drawing all these line segments, you get a lot of different regions of points! The model calculates the average of the points in each region and takes all the averages to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features compared to K-Nearest Neighbors:\n",
    "-Missing/imbalanced data does not affect this model as much as K-NN.\\\n",
    "-More time and complexity needed to train the model\\\n",
    "-It is less flexible to data changes: one change can completely change the tree structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have an idea of what the model is about, let's create the model and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_model = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits = 10)\n",
    "tree_score = abs(cross_val_score(tree_model,X,y, scoring = 'neg_root_mean_squared_error', cv=kfold))\n",
    "print(\"RMSE Average 10-Fold Cross Validation Error: {}\".format(tree_score.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters\n",
    "If you're interested, you can try tuning parameters such as `max_depth` and `min_samples_split` to improve the model score (check out the scikit-learn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)). `max_depth` sets the maximum depth of the tree, or how many times it should keep splitting the nodes before stopping. A deeper tree is more affected by the data set, so reduce `max_depth` to reduce overfitting, and increase it to reduce underfitting.\\\n",
    "You can also check out [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), an algorithm that uses multiple decision trees and compares all of their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. Now you know basic methods to use in Data Science and improve your model. Use any dataset that inspires you(and it is also completely fine to use the given dataset) and try to utilize the techniques that you learned to come up with the model that works well!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
